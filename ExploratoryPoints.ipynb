{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exploratory Points: Sentiment Analysis of Shakespeareâ€™s Plays\n",
    "\n",
    "\n",
    "In this part we executed sentiment analysis of Shakespeare. <br /><br />\n",
    "We made use of the AFINN library (https://github.com/fnielsen/afinn) which implements a dictionary-based approach to sentiment analysis - based on \"AFINN Lexicon\". The AFINN lexicon contains a list of words with corresponding valence values.<br />\n",
    "Some of the algorithms for which <b>we entirely wrote our own code</b> are based on the senetiment anaylsis concepts presented in the paper \"Nalisnick, Eric T., and Henry S. Baird. \"Extracting Sentiment Networks from Shakespeare's Plays.\" 2013 12th International Conference on Document Analysis and Recognition. IEEE, 2013.\"<br />\n",
    "Other algorithms like those for longitudinal sentiment analysis of individual plays trough time we developed entirely on our own.<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing neccessary libraries.<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ColorConverter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Using AFINN library - pip install afinn\n",
    "from afinn import Afinn\n",
    "\n",
    "# Load the NLTK punkt tokenizer - uncomment the next line if you don't have the punkt tokenizer \n",
    "# installed - it will open a dialog that will allow you to do so.\n",
    "#nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# special matplotlib command for global plot configuration\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "dark2_colors = ['#1b9e77','#d95f02','#7570b3','#e7298a','#66a61e','#e6ab02','#a6761d','#666666']\n",
    "set2_colors = ['#66c2a5','#fc8d62','#8da0cb','#e78ac3','#a6d854','#ffd92f','#e5c494','#b3b3b3']\n",
    "\n",
    "bold_colors = ['#FF0000', '#00FF00', '#FFAAAA', '#0000FF']\n",
    "light_colors = ['#FFAAAA', '#AAFFAA', '#AAAAFF', '#00FF00']\n",
    "cmap_light = ListedColormap(light_colors)\n",
    "cmap_bold = ListedColormap(bold_colors)\n",
    "dark2_cmap = ListedColormap(dark2_colors)\n",
    "set2_cmap = ListedColormap(dark2_colors)\n",
    "light_grey = np.array([float(248)/float(255)]*3)\n",
    "shade_black = '#262626'\n",
    "\n",
    "def set_mpl_params():\n",
    "    rcParams['figure.figsize'] = (10, 6)\n",
    "    rcParams['figure.dpi'] = 150\n",
    "    rcParams['axes.prop_cycle'].by_key()['color'][1]\n",
    "    rcParams['lines.linewidth'] = 2\n",
    "    rcParams['axes.facecolor'] = 'white'\n",
    "    rcParams['font.size'] = 16\n",
    "    rcParams['patch.edgecolor'] = 'white'\n",
    "    rcParams['patch.facecolor'] = dark2_colors[0]\n",
    "    rcParams['font.family'] = 'StixGeneral'\n",
    "\n",
    "set_mpl_params()\n",
    "\n",
    "# Remove border function taken from CS109 2013 \n",
    "######################################\n",
    "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
    "    \"\"\"\n",
    "    Minimize chartjunk by stripping out unnecessary plot borders and axis ticks\n",
    "    \n",
    "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
    "    \"\"\"\n",
    "    ax = axes or plt.gca()\n",
    "    ax.spines['top'].set_visible(top)\n",
    "    ax.spines['right'].set_visible(right)\n",
    "    ax.spines['left'].set_visible(left)\n",
    "    ax.spines['bottom'].set_visible(bottom)\n",
    "    \n",
    "    #turn off all ticks\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    \n",
    "    #now re-enable visibles\n",
    "    if top:\n",
    "        ax.xaxis.tick_top()\n",
    "    if bottom:\n",
    "        ax.xaxis.tick_bottom()\n",
    "    if left:\n",
    "        ax.yaxis.tick_left()\n",
    "    if right:\n",
    "        ax.yaxis.tick_right()\n",
    "        \n",
    "def cible_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
    "    \"\"\"\n",
    "    Make a target axis at 0,0 with ticks along the axis lines\n",
    "    \n",
    "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
    "    \"\"\"\n",
    "    ax = axes or plt.gca()\n",
    "\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.spines['bottom'].set_position(('data',0))\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.spines['left'].set_position(('data',0))\n",
    "\n",
    "    #now re-enable visibles\n",
    "    if top:\n",
    "        ax.xaxis.tick_top()\n",
    "    if bottom:\n",
    "        ax.xaxis.tick_bottom()\n",
    "    if left:\n",
    "        ax.yaxis.tick_left()\n",
    "    if right:\n",
    "        ax.yaxis.tick_right()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Reading in Shakespeare's plays:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in Shakespeare's plays\n",
    "df = pd.read_pickle('data.pickle')\n",
    "\n",
    "afinn = Afinn()\n",
    "\n",
    "# For every utterance, add a Valence score\n",
    "afinn_scores = [afinn.score(text) for text in df.Utterance]\n",
    "\n",
    "df['afinn'] = afinn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Extracting the text of'Hamlet':</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_hamlet = df[df.Play==\"hamlet\"]\n",
    "#df_hamlet.shape\n",
    "\n",
    "alltext = ''\n",
    "for text in df_hamlet['Utterance']:\n",
    "    alltext += text + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Removing Punctuation and Tokenizing :</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translator = str.maketrans({key: None for key in string.punctuation})\n",
    "no_punctuation = alltext.translate(translator)\n",
    "tokens = nltk.word_tokenize(no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a \"sliding window\". We initially set the sliding window to encompass first N=500 words of the play. We then \"move\" this window, word-by-word, troughout the play and calculate the sentiment score for the words in each of these windows. This allows us to obtain a fine-grained analysis of the sentiments.<br />\n",
    "<br />\n",
    "For example, the following text:<br /> \n",
    "> \"Rob Haris doing Shakespeare sentiment analysis\" \n",
    "\n",
    "with a sliding window of N=3 would give us the following sliding-window instances:<br />\n",
    "\n",
    "> Rob Haris doing\n",
    "\n",
    "> Haris doing Shakespeare\n",
    "\n",
    "> doing Shakespeare sentiment\n",
    "\n",
    "> Shakespeare sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 500\n",
    "grams = [tokens[i:i+N] for i in range(len(tokens)-N+1)]\n",
    "\n",
    "for i in range (len(grams)):\n",
    "    grams[i] = \" \".join(grams[i])\n",
    "    \n",
    "\n",
    "grams_df = pd.DataFrame(grams)\n",
    "grams_df = grams_df.rename(columns = {0:'Speech'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate sentiment scores for each othe sliding-window instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "afinn_scores = [afinn.score(text) for text in grams_df.Speech]\n",
    "\n",
    "grams_df['afinn'] = afinn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Plotting the sentiment movement troughout the play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "grams_df['afinn'].plot()\n",
    "\n",
    "plt.title('Longitudinal sentiment analysis of play \"Hamlet\" (word-window=500)')\n",
    "plt.xlabel('position in the play')\n",
    "plt.ylabel('sentiment score')\n",
    "remove_border()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Re-read in Shakespeare's plays\n",
    "df = pd.read_pickle('data.pickle')\n",
    "\n",
    "afinn = Afinn()\n",
    "\n",
    "# For every utterance, add a Valence score\n",
    "afinn_scores = [afinn.score(text) for text in df.Utterance]\n",
    "\n",
    "df['afinn'] = afinn_scores\n",
    "\n",
    "#################################\n",
    "\n",
    "df_hamlet = df[df.Play==\"hamlet\"]\n",
    "df_hamlet.shape\n",
    "\n",
    "alltext = ''\n",
    "for text in df_hamlet['Utterance']:\n",
    "    alltext += text + ' '\n",
    "\n",
    "#################################\n",
    "\n",
    "translator = str.maketrans({key: None for key in string.punctuation})\n",
    "no_punctuation = alltext.translate(translator)\n",
    "tokens = nltk.word_tokenize(no_punctuation)\n",
    "\n",
    "#################################\n",
    "\n",
    "N = 100\n",
    "grams = [tokens[i:i+N] for i in range(len(tokens)-N+1)]\n",
    "\n",
    "for i in range (len(grams)):\n",
    "    grams[i] = \" \".join(grams[i])\n",
    "    \n",
    "\n",
    "grams_df = pd.DataFrame(grams)\n",
    "grams_df = grams_df.rename(columns = {0:'Speech'})\n",
    "\n",
    "#################################\n",
    "\n",
    "afinn_scores = [afinn.score(text) for text in grams_df.Speech]\n",
    "\n",
    "grams_df['afinn'] = afinn_scores\n",
    "\n",
    "#################################\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "grams_df['afinn'].plot()\n",
    "\n",
    "plt.title('Longitudinal sentiment analysis of play \"Hamlet\" (word-window=100)')\n",
    "plt.xlabel('position in the play')\n",
    "plt.ylabel('sentiment score')\n",
    "remove_border()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in Shakespeare's plays\n",
    "df = pd.read_pickle('data.pickle')\n",
    "\n",
    "afinn = Afinn()\n",
    "\n",
    "# For every utterance, add a Valence score\n",
    "afinn_scores = [afinn.score(text) for text in df.Utterance]\n",
    "\n",
    "df['afinn'] = afinn_scores\n",
    "\n",
    "#################################\n",
    "\n",
    "df_hamlet = df[df.Play==\"hamlet\"]\n",
    "df_hamlet.shape\n",
    "\n",
    "alltext = ''\n",
    "for text in df_hamlet['Utterance']:\n",
    "    alltext += text + ' '\n",
    "\n",
    "#################################\n",
    "\n",
    "translator = str.maketrans({key: None for key in string.punctuation})\n",
    "no_punctuation = alltext.translate(translator)\n",
    "tokens = nltk.word_tokenize(no_punctuation)\n",
    "\n",
    "#################################\n",
    "\n",
    "N = 1000\n",
    "grams = [tokens[i:i+N] for i in range(len(tokens)-N+1)]\n",
    "\n",
    "for i in range (len(grams)):\n",
    "    grams[i] = \" \".join(grams[i])\n",
    "    \n",
    "\n",
    "grams_df = pd.DataFrame(grams)\n",
    "grams_df = grams_df.rename(columns = {0:'Speech'})\n",
    "\n",
    "#################################\n",
    "\n",
    "afinn_scores = [afinn.score(text) for text in grams_df.Speech]\n",
    "\n",
    "grams_df['afinn'] = afinn_scores\n",
    "\n",
    "#################################\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "grams_df['afinn'].plot()\n",
    "\n",
    "plt.title('Longitudinal sentiment analysis of play \"Hamlet\" (word-window=1000)')\n",
    "plt.xlabel('position in the play')\n",
    "plt.ylabel('sentiment score')\n",
    "remove_border()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in Shakespeare's plays\n",
    "df = pd.read_pickle('data.pickle')\n",
    "\n",
    "afinn = Afinn()\n",
    "\n",
    "# For every utterance, add a Valence score\n",
    "afinn_scores = [afinn.score(text) for text in df.Utterance]\n",
    "\n",
    "df['afinn'] = afinn_scores\n",
    "\n",
    "#################################\n",
    "\n",
    "df_hamlet = df[df.Play==\"r_and_j\"]\n",
    "df_hamlet.shape\n",
    "\n",
    "alltext = ''\n",
    "for text in df_hamlet['Utterance']:\n",
    "    alltext += text + ' '\n",
    "\n",
    "#################################\n",
    "\n",
    "translator = str.maketrans({key: None for key in string.punctuation})\n",
    "no_punctuation = alltext.translate(translator)\n",
    "tokens = nltk.word_tokenize(no_punctuation)\n",
    "\n",
    "#################################\n",
    "\n",
    "N = 1000\n",
    "grams = [tokens[i:i+N] for i in range(len(tokens)-N+1)]\n",
    "\n",
    "for i in range (len(grams)):\n",
    "    grams[i] = \" \".join(grams[i])\n",
    "    \n",
    "\n",
    "grams_df = pd.DataFrame(grams)\n",
    "grams_df = grams_df.rename(columns = {0:'Speech'})\n",
    "\n",
    "#################################\n",
    "\n",
    "afinn_scores = [afinn.score(text) for text in grams_df.Speech]\n",
    "\n",
    "grams_df['afinn'] = afinn_scores\n",
    "\n",
    "#################################\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "grams_df['afinn'].plot()\n",
    "\n",
    "plt.title('Longitudinal sentiment analysis of play \"Romeo and Juliet\" (word-window=1000)')\n",
    "plt.xlabel('position in the play')\n",
    "plt.ylabel('sentiment score')\n",
    "remove_border()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in Shakespeare's plays\n",
    "df = pd.read_pickle('data.pickle')\n",
    "\n",
    "afinn = Afinn()\n",
    "\n",
    "# For every utterance, add a Valence score\n",
    "afinn_scores = [afinn.score(text) for text in df.Utterance]\n",
    "\n",
    "df['afinn'] = afinn_scores\n",
    "\n",
    "#################################\n",
    "\n",
    "df_hamlet = df[df.Play==\"taming\"]\n",
    "df_hamlet.shape\n",
    "\n",
    "alltext = ''\n",
    "for text in df_hamlet['Utterance']:\n",
    "    alltext += text + ' '\n",
    "\n",
    "#################################\n",
    "\n",
    "translator = str.maketrans({key: None for key in string.punctuation})\n",
    "no_punctuation = alltext.translate(translator)\n",
    "tokens = nltk.word_tokenize(no_punctuation)\n",
    "\n",
    "#################################\n",
    "\n",
    "N = 1000\n",
    "grams = [tokens[i:i+N] for i in range(len(tokens)-N+1)]\n",
    "\n",
    "for i in range (len(grams)):\n",
    "    grams[i] = \" \".join(grams[i])\n",
    "    \n",
    "\n",
    "grams_df = pd.DataFrame(grams)\n",
    "grams_df = grams_df.rename(columns = {0:'Speech'})\n",
    "\n",
    "#################################\n",
    "\n",
    "afinn_scores = [afinn.score(text) for text in grams_df.Speech]\n",
    "\n",
    "grams_df['afinn'] = afinn_scores\n",
    "\n",
    "#################################\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "grams_df['afinn'].plot()\n",
    "\n",
    "plt.title('Longitudinal sentiment analysis of play \"Taming\" (word-window=1000)')\n",
    "plt.xlabel('position in the play')\n",
    "plt.ylabel('sentiment score')\n",
    "remove_border()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiating comedies from tragedies\n",
    "\n",
    "The paper first attempts to differentiate comedies from tragedies based on the summed valence for each play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "awv = {}\n",
    "\n",
    "for play in df.Play.unique():\n",
    "    dfp = df[df['Play']==play]\n",
    "\n",
    "    # Sum the valences for each of the utterances\n",
    "    sum_valence = dfp.afinn.sum()\n",
    "    \n",
    "    # Generate one big string for the full play\n",
    "    alltext = ''\n",
    "    for text in dfp['Utterance']:\n",
    "        alltext += text + ' '\n",
    "    \n",
    "    \n",
    "    # The following lines can be added if you wish to take into account the play's length\n",
    "    #\n",
    "    \n",
    "    # Create a dictionary using a comprehension - this maps every character from\n",
    "    # string.punctuation to None. Initialize a translation object from it.\n",
    "    #translator = str.maketrans({key: None for key in string.punctuation})\n",
    "\n",
    "    # Remove the punctuation using the translator\n",
    "    #no_punctuation = alltext.translate(translator)\n",
    "    \n",
    "    # Split the play into tokens\n",
    "    #tokens = nltk.word_tokenize(no_punctuation)\n",
    "\n",
    "    # Calculate the number of words in the play\n",
    "    #len_play = len(tokens)\n",
    "    \n",
    "    \n",
    "    # Find the number of words in AFINN that are in the play\n",
    "    words_in_afinn = afinn.find_all(alltext)\n",
    "\n",
    "\n",
    "    # Calculate the average word valence\n",
    "    # The paper seems to infer there are two methods to calculate it:\n",
    "    # i) \"Fig. 1. For each play the valence for each word was summed and then divided by the number\n",
    "    # of words in both the word list (AFINN) and the respective play.\"\n",
    "    # ii) \"...  by summing the valence values for all words in the play and then dividing by the number \n",
    "    # of the playâ€™s words in AFINN\"\n",
    "    #\n",
    "    # We've chosen the later.\n",
    "    awv[play] = (sum_valence / len(words_in_afinn)) \n",
    "\n",
    "    \n",
    "# Define the list of tragedies and comedies in the order given in the paper\n",
    "tragedies = ['titus','r_and_j', 'j_caesar', 'hamlet', 'othello', 'timon', 'lear', 'macbeth', 'a_and_c', 'coriolan']\n",
    "comedies = ['all_well', 'as_you', 'com_err', 'lll', 'm_for_m', 'merchant', 'm_wives', 'dream', 'much_ado', 'pericles',\n",
    "           'taming', 't_night', 'two_gent', 'win_tale']\n",
    "all_labels = comedies + tragedies \n",
    "\n",
    "awv_sorted = OrderedDict()\n",
    "for name in comedies:\n",
    "    awv_sorted[name] = awv[name]\n",
    "    \n",
    "for name in tragedies:\n",
    "    awv_sorted[name] = awv[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16,9)\n",
    "\n",
    "bar=plt.bar(range(len(awv_sorted)), awv_sorted.values(), align='center', color='blue')\n",
    "\n",
    "# Change the color of the tragedies to red\n",
    "for i in range(len(comedies),len(comedies)+len(tragedies)):\n",
    "    bar[i].set_color('red')\n",
    "\n",
    "plt.xticks(range(len(all_labels)), all_labels)\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-to-Character Analysis\n",
    "\n",
    "The paper next attempts to determine the sentiment between characters. The character-to-character sentiment is determined by \"summing the valence values over each instance of continuous speech and then assumed that sentiment was directed towards the character that spoke immediately before the current speaker.\" Our data structure is ordered by speaker and so we can simply iterate through it and calculate the valence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from_character = {}\n",
    "speaker = ''\n",
    "previous_speaker = ''\n",
    "\n",
    "# We'll look at character interactions in Hamlet\n",
    "dfp = df[df.Play=='hamlet']\n",
    "\n",
    "# Iterate through all Utterances\n",
    "for i, r in dfp.iterrows():\n",
    "\n",
    "    # Determine who is speaking\n",
    "    speaker = r['Speaker']\n",
    "\n",
    "    # Only calculate if someone spoke before\n",
    "    if len(previous_speaker) > 0:\n",
    "        # Get the list of speakers that this speaker has spoken to\n",
    "        if speaker in from_character:\n",
    "            to_character = from_character[speaker]\n",
    "        else:\n",
    "            to_character = {}\n",
    "            from_character[speaker] = to_character\n",
    "\n",
    "        # Get the running total of sentiment valence so far and add the new valence\n",
    "        if previous_speaker in to_character:\n",
    "            to_character[previous_speaker] += r['afinn']\n",
    "        else:\n",
    "            to_character[previous_speaker] = r['afinn']\n",
    "\n",
    "    # The current speaker will become the previous speaker for the next utterance\n",
    "    previous_speaker = speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out Hamlet's Sentiment Valence Sum \n",
    "from IPython.display import display, HTML\n",
    "df_print = pd.DataFrame(list(from_character['HAMLET'].items()), columns=['Speaker', 'Valence'])\n",
    "\n",
    "df_print.sort_values('Valence',inplace=True, ascending=False)\n",
    "display(df_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out Hamlet's Sentiment Valence Sum \n",
    "\n",
    "df_print2 = pd.DataFrame(list(from_character['GUILDENSTERN'].items()), columns=['Speaker', 'Valence'])\n",
    "\n",
    "df_print2.sort_values('Valence',inplace=True, ascending=False)\n",
    "display(df_print2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G=nx.Graph()\n",
    "\n",
    "for i, r in df_print.iterrows():\n",
    "    G.add_edge('HAMLET',r['Speaker'],weight=r['Valence'])\n",
    "\n",
    "\n",
    "elarge=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] >0]\n",
    "esmall=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight'] <=0]\n",
    "\n",
    "pos=nx.spring_layout(G) # positions for all nodes\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G,pos,node_size=500)\n",
    "\n",
    "# edges\n",
    "nx.draw_networkx_edges(G,pos,edgelist=elarge,\n",
    "                    width=6)\n",
    "nx.draw_networkx_edges(G,pos,edgelist=esmall,\n",
    "                    width=6,alpha=0.5,edge_color='b',style='dashed')\n",
    "\n",
    "# labels\n",
    "nx.draw_networkx_labels(G,pos,font_size=10,font_family='sans-serif')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show() # display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
